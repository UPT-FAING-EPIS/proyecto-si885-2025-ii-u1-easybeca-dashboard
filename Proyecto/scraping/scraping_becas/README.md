# API de Scraping de Becas PerÃº

Sistema de web scraping desarrollado en Python con FastAPI para extraer, validar y comparar informaciÃ³n de becas de estudio de fuentes oficiales en PerÃº.

## ğŸ¯ CaracterÃ­sticas Principales

- **Scraping Automatizado**: Extrae datos de PRONABEC, universidades y BCP
- **API REST**: Endpoints para controlar el scraping y acceder a los datos
- **Interfaz Web**: Dashboard para monitorear el proceso y visualizar resultados
- **ValidaciÃ³n de Datos**: Compara datos scrapeados con archivo Excel de referencia
- **Base de Datos**: Almacenamiento persistente con SQLite
- **ExportaciÃ³n**: Datos disponibles en Excel y JSON

## ğŸ—ï¸ Estructura del Proyecto

```
scraping_becas/
â”œâ”€â”€ main.py                 # AplicaciÃ³n principal FastAPI
â”œâ”€â”€ requirements.txt        # Dependencias del proyecto
â”œâ”€â”€ README.md              # DocumentaciÃ³n
â”œâ”€â”€ scrapers/              # MÃ³dulos de scraping
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pronabec_scraper.py
â”‚   â”œâ”€â”€ universities_scraper.py
â”‚   â””â”€â”€ bcp_scraper.py
â”œâ”€â”€ database/              # GestiÃ³n de base de datos
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ database_manager.py
â”œâ”€â”€ utils/                 # Utilidades y helpers
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ helpers.py
â”œâ”€â”€ templates/             # Plantillas HTML
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ sources.html
â”‚   â””â”€â”€ comparison.html
â””â”€â”€ static/               # Archivos estÃ¡ticos
    â”œâ”€â”€ style.css
    â””â”€â”€ script.js
```

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### Prerrequisitos

- Python 3.8 o superior
- Google Chrome (para Selenium)
- ChromeDriver

### InstalaciÃ³n

1. **Clonar o descargar el proyecto**
   ```bash
   cd scraping_becas
   ```

2. **Crear entorno virtual**
   ```bash
   python -m venv venv
   venv\Scripts\activate  # En Windows
   ```

3. **Instalar dependencias**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configurar ChromeDriver**
   - Descargar ChromeDriver desde https://chromedriver.chromium.org/
   - Agregar al PATH del sistema o colocar en el directorio del proyecto

## ğŸ® Uso

### Iniciar la AplicaciÃ³n

```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

La aplicaciÃ³n estarÃ¡ disponible en: http://localhost:8000

### Endpoints de la API

#### Scraping
- `POST /scrape/all` - Scrapear todas las fuentes
- `POST /scrape/pronabec` - Scrapear solo PRONABEC
- `POST /scrape/universities` - Scrapear solo universidades
- `POST /scrape/bcp` - Scrapear solo BCP

#### Datos
- `GET /data` - Obtener todos los datos scrapeados
- `GET /status` - Estado actual del scraping
- `GET /sources` - InformaciÃ³n de las fuentes

#### ComparaciÃ³n y ValidaciÃ³n
- `POST /compare` - Comparar con Excel de referencia
- `GET /validate` - Validar datos scrapeados

#### ExportaciÃ³n
- `GET /export/excel` - Exportar a Excel
- `GET /export/json` - Exportar a JSON

### Interfaz Web

1. **Dashboard Principal** (`/`)
   - Monitoreo en tiempo real
   - Controles de scraping
   - EstadÃ­sticas rÃ¡pidas

2. **Fuentes de Datos** (`/sources`)
   - InformaciÃ³n detallada de cada fuente
   - Estado de las fuentes
   - Controles especÃ­ficos

3. **ComparaciÃ³n** (`/comparison`)
   - Resultados de comparaciÃ³n con Excel
   - AnÃ¡lisis de coincidencias
   - MÃ©tricas de precisiÃ³n

## ğŸ“Š Fuentes de Datos

### PRONABEC
- **URL**: https://www.pronabec.gob.pe/
- **Datos**: Becas nacionales e internacionales
- **MÃ©todo**: Requests + BeautifulSoup + Selenium

### Universidades
- **Fuentes**: MÃºltiples universidades peruanas
- **Datos**: Becas institucionales y convenios
- **MÃ©todo**: Scraping adaptativo por universidad

### BCP (Banco de CrÃ©dito del PerÃº)
- **URL**: Programas de becas BCP
- **Datos**: Becas de excelencia acadÃ©mica
- **MÃ©todo**: Selenium para contenido dinÃ¡mico

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Variables de Entorno

Crear archivo `.env` (opcional):
```env
DATABASE_URL=sqlite:///./becas.db
CHROME_DRIVER_PATH=/path/to/chromedriver
LOG_LEVEL=INFO
SCRAPING_DELAY=2
MAX_RETRIES=3
```

### PersonalizaciÃ³n de Scrapers

Cada scraper puede configurarse modificando los archivos en `/scrapers/`:

- **URLs objetivo**: Modificar las listas de URLs
- **Selectores CSS**: Actualizar selectores para cambios en sitios web
- **Filtros**: Ajustar criterios de filtrado de datos
- **Validaciones**: Personalizar reglas de validaciÃ³n

## ğŸ“ˆ Monitoreo y Logs

### Logs del Sistema
- Los logs se guardan en `logs/scraping.log`
- Niveles: DEBUG, INFO, WARNING, ERROR
- RotaciÃ³n automÃ¡tica de archivos

### Base de Datos
- **Archivo**: `becas.db` (SQLite)
- **Tablas**: scholarships, excel_data, comparisons, scraping_logs
- **Backup**: AutomÃ¡tico antes de cada scraping

## ğŸ§ª Testing

### Ejecutar Tests
```bash
python -m pytest tests/ -v
```

### Tests Incluidos
- Tests unitarios para scrapers
- Tests de integraciÃ³n para API
- Tests de validaciÃ³n de datos
- Tests de comparaciÃ³n con Excel

## ğŸ”’ Consideraciones de Seguridad

- **Rate Limiting**: Delays entre requests para evitar bloqueos
- **User Agents**: RotaciÃ³n de user agents
- **Proxies**: Soporte para proxies (configuraciÃ³n manual)
- **Cookies**: GestiÃ³n automÃ¡tica de sesiones

## ğŸ› SoluciÃ³n de Problemas

### Errores Comunes

1. **ChromeDriver no encontrado**
   ```
   SoluciÃ³n: Verificar instalaciÃ³n y PATH de ChromeDriver
   ```

2. **Timeout en scraping**
   ```
   SoluciÃ³n: Aumentar delays en configuraciÃ³n
   ```

3. **Datos no encontrados**
   ```
   SoluciÃ³n: Verificar selectores CSS en scrapers
   ```

4. **Error de conexiÃ³n**
   ```
   SoluciÃ³n: Verificar conectividad y URLs objetivo
   ```

### Logs de Debug

Activar logs detallados:
```python
import logging
logging.getLogger().setLevel(logging.DEBUG)
```

## ğŸ“ ContribuciÃ³n

### Agregar Nueva Fuente

1. Crear nuevo scraper en `/scrapers/`
2. Implementar clase heredando de base scraper
3. Agregar endpoints en `main.py`
4. Actualizar interfaz web
5. Agregar tests correspondientes

### Mejoras Sugeridas

- [ ] Scraping programado (cron jobs)
- [ ] Notificaciones por email
- [ ] Dashboard de mÃ©tricas avanzadas
- [ ] API de webhooks
- [ ] Soporte para mÃ¡s formatos de exportaciÃ³n

## ğŸ“„ Licencia

Este proyecto estÃ¡ desarrollado para fines educativos y de investigaciÃ³n. Respetar los tÃ©rminos de uso de los sitios web scrapeados.

## ğŸ¤ Soporte

Para reportar problemas o solicitar caracterÃ­sticas:
1. Revisar logs del sistema
2. Verificar configuraciÃ³n
3. Consultar documentaciÃ³n de APIs
4. Contactar al equipo de desarrollo

---

**Nota**: Este sistema estÃ¡ diseÃ±ado para extraer informaciÃ³n pÃºblica de becas con fines de investigaciÃ³n y comparaciÃ³n. AsegÃºrate de cumplir con los tÃ©rminos de uso de cada sitio web.